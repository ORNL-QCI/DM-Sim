# DM_Sim: Quantum Circuit Simulator via Density Matrix on GPU Clusters

A Density Matrix Quantum Simulator for Single-GPU, Single-Node-Multi-GPUs and Multi-Nodes GPU Cluster. Please see our SuperComputing (SC-20) [paper](doc/paper_sc20.pdf) for details.

![alt text](img/example.png)

## Current version

Latest version: **1.0**

DM_Sim is still under active development. Please be patient if there's an error. We will continuously add new features. Questions and suggestions are welcome.

We will refine the design and implementation so that a new circuit can be directly launched and execcute it without the need to recompile (which is the current practice). This will benefit emerging quantum-classical hybrid algorithms where the quantum circuits are adjusted and executed online guided by classical optimization process. In the future version we will try to provide Python interface and support Q#/QDK.

## About DM_Sim

In this repository you will find a CUDA implementation for simulating deep quantum circuits on a single-GPU, a single-node-multi-GPUs (e.g., NVIDIA [DGX-1](https://www.nvidia.com/en-gb/data-center/dgx-systems/dgx-1/), [DGX-2](https://www.nvidia.com/en-us/data-center/dgx-2/) and HGX)), and multi-nodes GPU cluster (like the [Summit supercomputer](https://www.olcf.ornl.gov/summit/) in ORNL) using full density matrices. Our DM_sim simulator fully supports [OpenQASM](https://github.com/Qiskit/openqasm) intermediate-representation (IR) language (see [spec](https://arxiv.org/pdf/1707.03429.pdf). OpenQASM can be generated by Qiskit, Cirq, ProjectQ and Scaffold (see below). For scale-up (i.e., single-node-multi-GPUs), we leverage fast intra-node interconnects such as NVLink, NV-SLI and NVSwitch (see our [benchmarking](https://drive.google.com/open?id=13S4zbl4asHlWv_oebHw1YOWkzDr6I9wg) paper and [evaluation](https://arxiv.org/abs/1903.04611) paper about several modern GPU Interconnect). This simulator is based on the Multi-GPU-BSP (MG-BSP) model, please see our SuperComputing-20 paper for details.

DM_Sim simulates 1M general gates with 15-qubits gate-by-gate in 94 minutes on DGX-2 (16 NVIDIA V100 GPUs) using the density-operator -- on average 5.6 ms/gate.

## Supported Gate

|Gates | Meaning | Gates | Meaning |
|:---: | ------- | :---: | ------- |
|U3    | 3 parameter 2 pulse 1-qubit | CY | Controlled Y |
|U2 | 2 parameter 1 pulse 1-qubit  | SWAP | Swap     |
|U1 | 1 parameter 0 pulse 1-qubit  |  CH | Controlled H   |
|CX | Controlled-NOT  |  CCX | Toffoli   |
|ID | Idle gate or identity  |  CSWAP | Fredkin |
|X | Pauli-X bit flip  | CRX | Controlled RX rotation  |
|Y | Pauli-Y bit and phase flip  | CRY | Controlled RY rotation  |
|Z | Pauli-Z phase flip  |  CRZ | Controlled RZ rotation |
|H | Hadamard  |  CU1 | Controlled phase rotation  |
|S | sqrt(Z) phase  |  CU3 | Controlled U3 |
|SDG | conjugate of sqrt(Z)  |  RXX | 2-qubit XX rotation |
|T | sqrt(S) phase  | RZZ | 2-qubit ZZ rotation   |
|TDG | conjugate of sqrt(S) | RCCX | Relative-phase CXX |
|RX | X-axis rotation | RC3X | Relative-phase 3-controlled X |
|RY | Y-axis rotation | C3X | 3-controlled X |
|RZ | Z-axis rotation | C3XSQRTX | 3-controlled sqrt(X) |
|CZ | Controlled phase | C4X | 4-controlled X |
|C1 | Arbitrary 1-qubit gate | C2 | Arbitrary 2-qubit gate |

## Package Structure
#### **src**: DM_Sim source file
 - util.cuh: Error checking, allocation and release, and validation functions.
 - gate.cuh: Gate definition.
 - dmsim_sin.cu: single-GPU DM_Sim main simulation program.
 - dmsim_omp.cu: OpenMP-based single-node-multi-GPU main simulation program.
 - dmsim_mpi.cu: MPI-based multi-nodes main simulation program.
 - configuration_sample.h: DM_Sim configurations and gate macro.
 - circuit.cuh: Circuit file. It will be auto-generated by *dmsim_qasm_ass.py* and *dmsim_gen_cir.py*
 - summit_dmsim.lsf: Sample ORNL Summit lsf file for job submission.
 - Makefile_sample: sometimes used for generating Makefile.
 - Makefile: for compilation. It will be auto-generated by *dmsim_qasm_ass.py* and *dmsim_gen_cir.py*

#### **benchmark**: 
 - OpenQASM-based benchmarks (.qasm), it contains some circuits in the paper, for more, please refer to our [QASMBench](https://github.com/uuudown/QASMBench).

#### **tool**: Supporting tools (see our paper for detail).
 - dmsim_qasm_ass.py: Convert OpenQASM file (e.g., adder.qasm) to a DM_Sim circuit file (e.g., circuit.cuh). It also reports the total number of qubits, the total number of gates and the total number of CX (CNOT) gates.
 - dmsim_gen_cir.py: Generate synthetic circuits by (1) randomly sampling from a gate set; (2) general 1-qubit circuits (purely C1 gates).
 - dmsim_run_test.py: Run tests by generating synthetic circuits. Perform qubits, gates, GPU-scaling, and very-deep tests. It invokes dmsim_gen_cir.py. 
 - dmsim_run_benchmark.py: Run some benchmarks from the benchmark folder. It calls dmsim_qasm_ass.py for assembling.

#### **artifact**: System configuration for the evaluation performed in our paper.
These are generated by using the SC [Author-Kit](https://github.com/SC-Tech-Program/Author-Kit) tool.
 - SLI.txt: For the SLI-system with two RTX2080 GPUs connected by NV-SLI bridge.
 - dgx-1P.txt: For the Pascal architecture P100-DGX-1 with 8 GPUs connected by NVLink-V1.
 - dgx-1V.txt: For the Volta architecture V100-DGX-1 with 8 GPUs connected by NVLink-V2.
 - dgx-2.txt: For the Volta architecture DGX-2 with 16 GPUs connected by NVSwitch.
 - Summit.txt: For the Volta architecture ORNL summit supercomputer supported GPUDirect-RDMA.

#### **img**: images for the Repo.

## Configuration
You need to first config the "configuration.h" file to update *N_QUBITS* to the number of qubits in the circuit, and *GPU_SCALE* to the log2 of the number of GPUs (e.g., GPU_SCALE=0 for 1 GPU, 2 for 4 GPUs, 3 for 8 GPUs, etc.). Note that configuration.h can be generated by our tool-script based on *configuration_sample.cuh* although configuration.h is actually used inside.

```text
#define N_QUBITS 10
#define GPU_SCALE 0 
```

You may also need to update "Makefile" to set your NVCC path and GPU architecture (e.g., SM_60 for Pascal, SM_70 for Volta and SM_75 for Turing GPUs). Again, Makefile can be generated by *dmsim_qasm_ass.py* and *dmsim_gen_cir.py* based on Makefile_sample although Makefile is actually used.

```
CC = nvcc
FLAGS = -O3 -arch=sm_70 -rdc=true
LIBS = -lm
```

## Build
DM-Sim requires CUDA Driver and Runtime to build and execute. 

|  Dependency  | Version |
|:-----------: | ------- |
| CUDA-Toolkit | 8.0 or later |

To build the scale-up version, it requires OpenMP. To build the scale-out version, it needs MPI (we only tested IBM XL and Spectrum-MPI on Summit Supercomputer). 

 Then the following for compilation: 
```text
make 
```

When compiling for very deep circuit, you're suggested to use multi-processing, such as:
```text
make -j 16
```

## Execution

DM-Sim requires NVIDIA GPUs for execution. We have tested it on Tesla-P100 (Pascal, CC-6.0), Tesla-V100 (Volta, CC-7.0) and RTX2080 (Turing, CC-7.5). To run on scale-up workstations (e.g., DGX-1 and DGX-2), it needs all the GPUs to be directly connected by NVLink, NVSwitch or NV-SLI for all-to-all communication (when performing adjoint operation when transposing the density matrix)). Therefore, on DGX-1, it can use up to 4 GPUs (despite 8 in total) and provided they are directly interconnected, see our TPDS [Evaluation paper](https://arxiv.org/abs/1903.04611) on GPU interconnect for detail. For scale-out GPU clusters, it requires the support of [GPUDirect-RDMA](https://docs.nvidia.com/cuda/gpudirect-rdma/index.html) for direct GPU-memory access. On the ORNL Summit supercomputer, this can be enabled by *--smpiargs="-gpu"*. See the example [.lsf](src/summit_dmsim.lsf) file.

### Single GPU

Using the following command for execution:
```text
./dmsim_sin
```

### Scale-up

```text
./dmsim_omp
```

### Scale-out
This is the execution command on ORNL Summit supercomputer (1024 resource sets with 1024 MPI ranks, 1 GPU per rank) with GPUDirect-RDMA enabled.
```text
jsrun -n1024 -a1 -g1 -c1 --smpiargs="-gpu" ./dmsim_mpi
```

## Expected Output

When build and execute correctly, the default circuit file "circuit.cuh", which realizes a ripple-carry adder using 10-qubits in total on a single-GPU, should print out the following output:
```text
nqubits:10, ngpus:1, comp:23.121, comm:0.000, sim:23.121, mem:32.000

===============  Measurement (qubit=10, repetition=10) ================
Test-0: 1000000010
Test-1: 1000000010
Test-2: 1000000010
Test-3: 1000000010
Test-4: 1000000010
Test-5: 1000000010
Test-6: 1000000010
Test-7: 1000000010
Test-8: 1000000010
Test-9: 1000000010
```
The inputs are: carry-in cin = 0, A=0001, B=1111. The outputs are: B=B+A=0000, carry-out=1.

"**nqubits**" is the number of qubits simulated. "**ngpus**" is the number of GPUs utilized. "**comp**" is the computation time (ms) in the simulation. "**comm**" is the communication time (ms) in the simulation. "**sim**" is total simulation latency (ms). "**mem**" is the total GPU memory cost for all GPUs (MB). 

The measurement measures all qubits at once. "**repetition**" refers to the number of repeated measurements. You can configure the number of trials when calling "measurement()". The default value is 10 times. Note, to run the unit-test or benchmark test, you should comment off the measurement so that the performance results can be correctly parsed by the scripts.

## More Configurations

To simulate qubit-size larger than 15, the index is already larger than a normal unsigned integer, you need to define **idxtype** as "**unsigned long long**" in configuration.h.

When defining "CUDA_ERROR_CHECK", DM_Sim checks CUDA API error and kernel execution error.

When defining "RAND_INIT_DM", we do random initialization for the density matrix rather than zero.




## Performance
DM-Sim is bounded by GPU memory access bandwidth, and possibly by interconnect bandwidth. We use the [Roofline](https://en.wikipedia.org/wiki/Roofline_model) model to show the bound. The real sustainable bandwidth is profiled by using the [Roofline Toolkit](https://bitbucket.org/berkeleylab/cs-roofline-toolkit/src/master/) from LBNL. This following figure shows the Roofline model for the simulation on SLI, DGX-1P, DGX-1V and DGX-2 systems. See the files in the **[artifact](artifact)** folder. AI stands for arithmetic intensity for the DM simulation.
![alt text](img/roofline.png)

#### We show the performance of simulation by increasing the number of qubits (256 gates):
![alt text](img/qubits.png)

#### We show the performance of simulation by increasing the number of gates (14 qubits):
![alt text](img/qubits.png)

#### And performance bound on computation, memory access and communication:
![alt text](img/bound.png)

#### Performance for deep circuits on DGX-2 using 16 GPUs and 15 qubits using general 1-qubit gate(i.e., C1 gate):

|Gates | Computation | Communication | Simulation | Time/Gate |
|:---: | :---------: | :-----------: | :--------: | :-------: |
| 10K  |     53.8s   |     9.36ms    |   53.8s    |  5.38ms   |
| 100K |     558.0s  |     7.31ms    |   558.0s   |  5.58ms   |
|  1M  |    5645.5s  |     7.21ms    |   5645.5s  |  5.65ms   |

#### Performance on ORNL Summit supercomputer, the numbers on the bars indicate the number of GPUs utilized. For benchmarks, please see [QASMBench](https://arxiv.org/abs/2005.13018). Clearly, the communication overhead is much more significant than scale-up.
<img src="img/summit.png" width="500">

## Support Tools

#### dmsim_qasm_ass.py
To assemble an OpenQASM (e.g., adder.qasm) to a DM_Sim circuit file (e.g., circuit.cuh):

```text
python dmsim_qasm_ass.py -i adder.qasm -o circuit.cuh
```
It outputs "circuit.cuh", "Makefile", and reports the number of qubits, the number of gates, and the number of CX/CNOT gates.

You can use "-s" to select the mode: 'sin' for single-GPU, 'omp' for OpenMP scale-up, and 'mpi' for MPI scale-out):

```text
python dmsim_qasm_ass.py -i adder.qasm -o circuit.cuh -s omp
```

For deep circuits, nvcc can take a long time for compilation, to accelerate this process by multi-processing, we partition deep circuits into segments for separated compilation. You can specify the threshold for this partition. Please see our SC-20 paper for details. To set the threshold as 512 gates:

```text
python dmsim_qasm_ass.py -i adder.qasm -o circuit.cuh -s omp -t 512
```

Use "-h" to see the options.

#### dmsim_gen_cir.py
Generate synthetic circuits for simulation. It randomly picks a gate from a gate-collection defined in the file, and randomly applies to a qubit or two qubits. 

Generating 256 gates on 12 qubits:

```text
python dmsim_gen_cir.py -n 12 -g 256
```

You can use "-s" to select the mode: 'sin' for single-GPU, 'omp' for OpenMP scale-up, and 'mpi' for MPI scale-out):

```text
python dmsim_gen_cir.py -n 12 -g 256 -s mpi
```

You can specify the threshold for deep-circuit simulation:
```text
python dmsim_gen_cir.py -n 12 -g 256 -s mpi -t 1024
```

In our SC-20 paper, we discussed **general** gates, which is essentially C1 for 1-qubit. We can generate purely C1-based circuits for most general simulations.
```text
python dmsim_gen_cir.py -n 12 -g 256 -s mpi -t 1024 -r 1
```

You can also specify the random seed, please see the options using "-h"



#### dmsim_run_test.py
Performing tests using generated synthetic circuits. It calls dmsim_gen_cir.py to generate the circuits and Makefile. 

For gate-test, specify the starting gate volume and ending gate volume, each step we double the number of gates:
```text
python dmsim_run_test.py -st_ng 8 -ed_ng 1024
```

For qubit-test, specify the starting qubit number and ending qubit number, each step we add one more qubit:
```text
python dmsim_run_test.py -st_nq 4 -ed_nq 12
```

You can use "-s" to select the mode: 'sin' for single-GPU, 'omp' for OpenMP scale-up, and 'mpi' for MPI scale-out):
```text
python dmsim_run_test.py -st_nq 4 -ed_nq 12 -s omp
```



For GPU-test, define "N_GPUS" as the maximum number of GPUs, we start from 2 GPUs and each time we double the number of GPUs (this will be updated).

For deep-test, define "N_QUBITS" and "N_GPUS" inside, and update "gs" list in the file (this will be updated)


#### dmsim_run_benchmark.py
Run real quantum circuit benchmarks from the "[benchmark](benchmark)" folder. It calls dmsim_qasm_ass.py to convert, and then compile and run the circuit. 

You need to input the benchmark information into the file. Please see the comments in the source file for how to add and specify an experiment.

You can use "-s" to select the mode: 'sin' for single-GPU, 'omp' for OpenMP scale-up, and 'mpi' for MPI scale-out):
```text
python dmsim_run_benchmark.py -s omp
```

## More Benchmarks
We have developed an OpenQASM based benchmark suite called "[QASMBench](https://github.com/uuudown/QASMBench)" which provides more real quantum circuit benchmarks. Please see our [QASMBench](https://arxiv.org/abs/2005.13018) paper for details.


### OpenQASM

OpenQASM (Open Quantum Assembly Language) is a low-level quantum intermediate representation (IR) for quantum instructions, similar to the traditional *Hardware-Description-Language* (HDL) like Verilog and VHDL. OpenQASM is the open-source unified low-level assembly language for IBM quantum machines publically available on cloud that have been investigated and verified by many existing research works. Several popular quantum software frameworks use OpenQASM as one of their output-formats, including [Qiskit](https://github.com/Qiskit/qiskit), [Cirq](https://github.com/quantumlib/cirq), [Scaffold](https://github.com/epiqc/ScaffCC), [ProjectQ](https://github.com/ProjectQ-Framework/ProjectQ), etc.

#### Qiskit
The *Quantum Information Software Kit* ([Qiskit](https://github.com/Qiskit/qiskit)) is a quantum software developed by *IBM*. It is based on Python. OpenQASM can be generated from Qiskit via:
```text
QuantumCircuit.qasm()
```

#### Cirq
[Cirq](https://github.com/quantumlib/cirq) is a quantum software framework from *Google*. OpenQASM can be generated from Cirq (not fully compatible) via:
```text
cirq.Circuit.to_qasm()
```

#### Scaffold
[Scaffold](https://github.com/epiqc/ScaffCC) is a quantum programming language embedded in the C/C++ programming language based on the [LLVM](https://github.com/llvm/llvm-project) compiler toolchain. A Scaffold program can be compiled by [Scaffcc](https://arxiv.org/pdf/1507.01902.pdf) to OpenQASM via the "**-b**" compiler option.

#### ProjectQ
[ProjectQ](https://github.com/ProjectQ-Framework/ProjectQ) is a quantum software platform developed by *Steiger et al.* from ETH Zurich. The official website is [here](https://projectq.ch/). ProjectQ can generate OpenQASM when using IBM quantum machines as the backends:
```text
IBMBackend.get_qasm()
```

## Authors 

#### [Ang Li](http://www.angliphd.com/), Pacific Northwest National Laboratory (PNNL)

#### [Sriram Krishnamoorthy](https://hpc.pnl.gov/people/sriram/), Pacific Northwest National Laboratory (PNNL)



## Citation format

For research articles, please cite our paper:

<!--  - Ang Li, Sriram Krishnamoorthy, "QASMBench: A Low-level QASM Benchmark Suite for NISQ Evaluation and Simulation" [[arXiv:2005.13018]](https://arxiv.org/abs/2005.13018).

Bibtex:
```text
@article{li2020qasmbench,
    title={QASMBench: A Low-level QASM Benchmark Suite for NISQ Evaluation and Simulation},
    author={Li, Ang and Krishnamoorthy, Sriram},
    journal={arXiv preprint arXiv:2005.13018},
    year={2020}
}

``` 
-->


## License

This project is licensed under the BSD License, see [LICENSE](LICENSE) file for details.

## Acknowledgments

**PNNL-IPID: 31919-E, ECCN: EAR99, IR: PNNL-SA-143160**

We thank the SC-20 anonymous reviewers for their feedback and suggestions on our paper. This work was supported by PNNL's *Quantum Algorithms, Software, and Architectures* (QUASAR) LDRD Initiative. The Pacific Northwest National Laboratory (PNNL) is operated by Battelle for the U.S. Department of Energy (DOE) under contract DE-AC05-76RL01830. 

## Contributing

Please contact us If you'd like to contribute to DM_Sim.
